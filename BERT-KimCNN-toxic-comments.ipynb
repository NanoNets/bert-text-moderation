{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, id, text, labels=None):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_examples(train_file):\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    ids = train_df['id'].values\n",
    "    text = train_df['comment_text'].values\n",
    "    labels = train_df[train_df.columns[2:]].values\n",
    "    examples = []\n",
    "    for i in range(len(train_df)):\n",
    "        examples.append(InputExample(ids[i], text[i], labels=labels[i]))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_examples(examples, max_seq_len, tokenizer):\n",
    "    features = []\n",
    "    for i,example in enumerate(examples):\n",
    "        tokens = tokenizer.tokenize(example.text)\n",
    "        if len(tokens) > max_seq_len - 2:\n",
    "            tokens = tokens[:(max_seq_len - 2)]\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        padding = [0] * (max_seq_len - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        assert len(input_ids) == max_seq_len\n",
    "        assert len(input_mask) == max_seq_len\n",
    "        assert len(segment_ids) == max_seq_len\n",
    "        label_ids = [float(label) for label in example.labels]\n",
    "        features.append(InputFeatures(input_ids=input_ids,\n",
    "                                      input_mask=input_mask,\n",
    "                                      segment_ids=segment_ids,\n",
    "                                      label_ids=label_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_features(features):\n",
    "    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.float)\n",
    "    dataset = TensorDataset(input_ids,\n",
    "                            input_mask,\n",
    "                            segment_ids,\n",
    "                            label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KimCNN(nn.Module):\n",
    "    def __init__(self, embed_num, embed_dim, dropout=0.1, kernel_num=3, kernel_sizes=[2,3,4], num_labels=2):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.embed_num = embed_num\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = dropout\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.embed = nn.Embedding(self.embed_num, self.embed_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (k, self.embed_dim)) for k in self.kernel_sizes])\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.classifier = nn.Linear(len(self.kernel_sizes)*self.kernel_num, self.num_labels)\n",
    "        \n",
    "    def forward(self, inputs, labels=None):\n",
    "        output = inputs.unsqueeze(1)\n",
    "        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in self.convs]\n",
    "        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in output]\n",
    "        output = torch.cat(output, 1)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.classifier(output)\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(type='cuda')\n",
    "pretrained_weights = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "basemodel = BertModel.from_pretrained(pretrained_weights)\n",
    "basemodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "train_file = 'train.csv'\n",
    "train_examples = get_train_examples(train_file)\n",
    "train_features = get_features_from_examples(train_examples, seq_len, tokenizer)\n",
    "train_dataset = get_dataset_from_features(train_features)\n",
    "\n",
    "\n",
    "train_val_split = 0.1\n",
    "train_size = int(len(train_dataset)*(1-train_val_split))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch = 8\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KimCNN(\n",
       "  (embed): Embedding(256, 768)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 3, kernel_size=(2, 768), stride=(1, 1))\n",
       "    (1): Conv2d(1, 3, kernel_size=(3, 768), stride=(1, 1))\n",
       "    (2): Conv2d(1, 3, kernel_size=(4, 768), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=9, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_num = seq_len \n",
    "embed_dim = basemodel.config.hidden_size \n",
    "dropout = basemodel.config.hidden_dropout_prob\n",
    "kernel_num = 3\n",
    "kernel_sizes = [2,3,4]\n",
    "num_labels = 6\n",
    "\n",
    "model = KimCNN(embed_num, embed_dim, dropout=dropout, kernel_num=kernel_num, kernel_sizes=kernel_sizes, num_labels=num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------EPOCH #1-----------\n",
      "training...\n",
      "evaluating...\n",
      "ROC AUC per label:\n",
      "toxic :  0.9605486064463264\n",
      "severe_toxic :  0.9841929394445961\n",
      "obscene :  0.9621109584292289\n",
      "threat :  0.7886769265503675\n",
      "insult :  0.9427310938607746\n",
      "identity_hate :  0.905177301817393\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-5\n",
    "epochs = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('-----------EPOCH #{}-----------'.format(i+1))\n",
    "    print('training...')\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            inputs,_ = basemodel(input_ids, segment_ids, input_mask)\n",
    "        loss = model(inputs, label_ids)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()        \n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    print('evaluating...')\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        val_input_ids, val_input_mask, val_segment_ids, val_label_ids = batch\n",
    "        with torch.no_grad():\n",
    "            val_inputs,_ = basemodel(val_input_ids, val_segment_ids, val_input_mask)\n",
    "            logits = model(val_inputs)\n",
    "        y_true.append(val_label_ids)\n",
    "        y_pred.append(logits)\n",
    "\n",
    "    y_true = torch.cat(y_true, dim=0).float().cpu().detach().numpy()\n",
    "    y_pred = torch.cat(y_pred, dim=0).float().cpu().detach().numpy()\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    for i,label in enumerate(labels):\n",
    "        fpr[label], tpr[label], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "    print('ROC AUC per label:')\n",
    "    for label in labels:\n",
    "        print(label, ': ', roc_auc[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Comment:\n",
      "Mainly to JTD ( i think ) - I've just scanned through this discussion stuff having read the article. My initial impression was that this was going very kindly on the British indeed. It is admitted at some point in the discussion that there is a move among Irish historians away from previous over - simplifications of the matter - but isn't this move itself perhpas reactionary? I mean, Irish historians of this or any generation speak for their times, and, for lay readers like myself, the CONTEXT of this estimation of the role of the British imperial adventure in the famine is lost to those who have not studied so carefully the history of the history of the famine. This is, afterall, a general encyclopedia. So please twist the screws a bit. I won't go editing the article, but some feedback would be appreciated in this regard, Basically, you're all better historians than myself, but maybe some of your nuances are therefore misplaced here, ( a bit less d'Alembert and more Diderot as it were ) Cheers, Simon\n",
      "Prediction:\n",
      "toxic :  0.009333727\n",
      "severe_toxic :  0.00015343125\n",
      "obscene :  0.020101553\n",
      "threat :  0.00029461284\n",
      "insult :  0.0016380053\n",
      "identity_hate :  0.0008575509\n",
      "---------------------------------\n",
      "Comment:\n",
      "Need some guidance I need to report another user for attempting to intimidate me by violating my privacy by calling out personal details about my location on my user talk page. His user name is Hasteur. Can you please direct me to the right place?\n",
      "Prediction:\n",
      "toxic :  0.0068807625\n",
      "severe_toxic :  0.00023090332\n",
      "obscene :  0.0145177\n",
      "threat :  0.00049068616\n",
      "insult :  0.005037717\n",
      "identity_hate :  0.0010609339\n",
      "---------------------------------\n",
      "Comment:\n",
      "I understand. I'm the only one left. I'm going to check out Centiare too. Looks like it will be way better than WP.\n",
      "Prediction:\n",
      "toxic :  0.13027665\n",
      "severe_toxic :  0.011313486\n",
      "obscene :  0.079457335\n",
      "threat :  0.004723253\n",
      "insult :  0.099267125\n",
      "identity_hate :  0.018787261\n",
      "---------------------------------\n",
      "Comment:\n",
      "Wikipedia : Notability clearly lists the criteria for article notability. Once a CMS has its own article, it can appear on this list - WP : WTAF. The page gets locked because people ignore the criteria. The policy is simple and clear. Trying to create separate criteria, as per cmsguy123's comment below, is not simple or clear.\n",
      "Prediction:\n",
      "toxic :  0.009161079\n",
      "severe_toxic :  0.00031898668\n",
      "obscene :  0.019232668\n",
      "threat :  0.0011332597\n",
      "insult :  0.0045801033\n",
      "identity_hate :  0.0020524412\n",
      "---------------------------------\n",
      "Comment:\n",
      "A redirect somewhere couldn't hurt, though I still don't think any o the candidates are ideal. But then if he passes the notability threshold some day the basic material is still there to work with.\n",
      "Prediction:\n",
      "toxic :  0.022079397\n",
      "severe_toxic :  0.0011208754\n",
      "obscene :  0.032582033\n",
      "threat :  0.002826403\n",
      "insult :  0.011208827\n",
      "identity_hate :  0.005050944\n",
      "---------------------------------\n",
      "Comment:\n",
      "What on earth does anti - semitism have to do with the notion that the US military shot down the plane, Cberlet? I am quite mystified.\n",
      "Prediction:\n",
      "toxic :  0.055252425\n",
      "severe_toxic :  0.002406106\n",
      "obscene :  0.04549172\n",
      "threat :  0.0027631551\n",
      "insult :  0.024192393\n",
      "identity_hate :  0.0069899107\n",
      "---------------------------------\n",
      "Comment:\n",
      "WikiAwards Hi there. Just to thank you for being the 2nd participant in the WikiAwards Project. Unfortunately the project is causing some controversy and I need to know if it is worth going on. I respect your neutrality but for now I really need to know if there's anybody in this world that supports my idea. I truly believe it is a great way of developing Wikipedia. 05 : 36, 27 Aug 2004 ( UTC )\n",
      "Prediction:\n",
      "toxic :  0.0039544455\n",
      "severe_toxic :  9.456437e-05\n",
      "obscene :  0.009531345\n",
      "threat :  0.00067216606\n",
      "insult :  0.0016024308\n",
      "identity_hate :  0.0007145586\n",
      "---------------------------------\n",
      "Comment:\n",
      "I'm afraid not. I think you need to ask somebody who's an administrator on Commons. ( I'm an admin on en. wiki, not on Commons. ) — Talk / Stalk\n",
      "Prediction:\n",
      "toxic :  0.017199917\n",
      "severe_toxic :  0.0008454597\n",
      "obscene :  0.025758682\n",
      "threat :  0.002444282\n",
      "insult :  0.008249491\n",
      "identity_hate :  0.0033726753\n",
      "---------------------------------\n",
      "Comment:\n",
      "Hey useless troll, you just don't get tired enough of being stomped again and again, right?\n",
      "Prediction:\n",
      "toxic :  0.7409974\n",
      "severe_toxic :  0.098911814\n",
      "obscene :  0.21756598\n",
      "threat :  0.007269864\n",
      "insult :  0.27900904\n",
      "identity_hate :  0.04931407\n",
      "---------------------------------\n",
      "Comment:\n",
      "= = Rambling, discoursive article There was evidently an edit war on this page a number of years ago, although I have trouble understanding exactly what it was about. As a result, the article reads very poorly now. There is, for instance, no clear summary of the timeline, e. g., how many months did the Satyagraha last? I'm not in a position to sort out what happened ( knowing very little of the history ), but I hope someone can step up and tidy this up.\n",
      "Prediction:\n",
      "toxic :  0.007893627\n",
      "severe_toxic :  0.0002704348\n",
      "obscene :  0.015856728\n",
      "threat :  0.0005324769\n",
      "insult :  0.006508061\n",
      "identity_hate :  0.001252369\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1.0 / (1.0 + np.exp(-1.0 * z))\n",
    "    return s\n",
    "\n",
    "for i in np.random.randint(0, len(y_pred), size=10):\n",
    "    string = tokenizer.decode(val_dataset[i][0], skip_special_tokens=True)\n",
    "    print('---------------------------------')\n",
    "    print('Comment:')\n",
    "    print(string)\n",
    "    preds = dict(zip(labels, sigmoid(y_pred[i])))\n",
    "    print('Prediction:')\n",
    "    for label in preds:\n",
    "        print(label, ': ', preds[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
